{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD_M73qh8FVV"
      },
      "source": [
        "# Introduction to Sentiment Analysis\n",
        "\n",
        "We will use the Python library '__TextBlob__' for this workshop. There are other alternatives but TextBlob provides all the basic functionality is relatively easy to learn.\n",
        "\n",
        "__TextBlob__ contains a pre-defined dictionary classifying negative and positive words. It works by analysing a given text and assigning individual scores to all the words it recognizes in a text. The final sentiment is calculated by  taking an average of all the individual sentiment scores. The range is from -1 (very negative) to +1 (very positive). \n",
        "\n",
        "## Install TextBlob and other libraries\n",
        "\n",
        "The first thing we need to do is install the necesssary Python libraries - this is straightforward using PIP (Python's package manager). Run this code block to install them. If you get a message, 'Requirement already satisfied', you can move on to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WRy7q6M8FVY"
      },
      "outputs": [],
      "source": [
        "# Run this code block the first time you use this Notebook\n",
        "!pip install textblob\n",
        "!pip install nltk\n",
        "!pip install matplotlib\n",
        "!pip install pandas  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIb-xn7P8FVd"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "#import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "import re\n",
        "import urllib.request\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ESFTs3j8FVa"
      },
      "source": [
        "## Analyse some Text\n",
        "\n",
        "In this example we:\n",
        "\n",
        "- provide a small fragment of text\n",
        "- assign the text to a variable (a temporary container for holding the text)\n",
        "- pass that variable to TextBlob.\n",
        "\n",
        "TextBlob will then provide a result.\n",
        "\n",
        "Please note in the code below some text is prepended with #. This tells Python not to process this text so we can use it to provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0ypoTl88FVa"
      },
      "outputs": [],
      "source": [
        "input_text = \"I think that big tech is doing a horrible thing for our country. And I believe it is going to be a catastrophic mistake for them.\" #(Donald Trump, Nov 13, 2020)\n",
        "\n",
        "# input_text = \"This is a terrific book, the writing is superb, overall an excellent read\" # (Amazon review)\n",
        "\n",
        "blob = TextBlob(input_text) #pass the input text to Textblob\n",
        "\n",
        "polarity = blob.sentiment.polarity #get a polarity score\n",
        "\n",
        "print(polarity) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change the input text\n",
        "\n",
        "The first text should have got a result of -0.5 which is quite negative. To try this again with a different piece of text, comment out the second line of code containing the input_text (place a # at the beginning of the line) and remove the # at the beginning of the 3rd line of code.\n",
        "\n",
        "You should now get a score of 0.5 which is positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Word About Functions\n",
        "\n",
        "However, getting our code to run on different data by changing what code is commented out can get messy and confusing. We can improve the structure of our code by organising it into functions. A function is a block of code that takes some input value and generates either returns an output value that can then be used by other code, or produces some behaviour like printing some text or saving a file. A good function should be kept as simple as possible and should do exactly one job. Organising your code this way makes it easier to re-use, update, and debug.\n",
        "\n",
        "A simple function definition has the following structure:\n",
        "\n",
        "```\n",
        "def get_polarity(text)\n",
        "    # code goes here\n",
        "    return output \n",
        "```\n",
        "\n",
        "- The function declaration\n",
        "    - `def` - keyword indicating what follows is a function declaration\n",
        "    - function name, e.g.: `get_polarity` - this is used to call the function once it has been declared\n",
        "    - arguments, e.g. `text` - this creates a special variable that only exists within the code of the function body. It is assigned a value when the function is called, which allows it to be called with different values at different times. If more than one argument is needed, they can be separated by commas\n",
        "- The function body - the actual code that defines the function behaviour, including...\n",
        "    - a return statement, e.g. `return polarity` - determines the value that the function outputs, which can then be used in other\n",
        "    code as, e.g., a value for a variable, an input to another function, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text1 = \"I think that big tech is doing a horrible thing for our country. And I believe it is going to be a catastrophic mistake for them.\" #(Donald Trump, Nov 13, 2020)\n",
        "input_text2 = \"This is a terrific book, the writing is superb, overall an excellent read\" # (Amazon review)\n",
        "\n",
        "\n",
        "def get_polarity(text):\n",
        "    blob = TextBlob(text) # Same code as before, but replace the variable with the function argument `text`\n",
        "    polarity = blob.sentiment.polarity \n",
        "    return polarity\n",
        "\n",
        "print('text 1 polarity:', get_polarity(input_text1))\n",
        "print('text 2 polarity:', get_polarity(input_text2)) \n",
        "\n",
        "# Bonus task: what's the most negative sentence you can make? The most \n",
        "# positive? The closest to exactly neutral? What does this task tell \n",
        "# us about how the scores are calculated?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te5G3Zf78FVc"
      },
      "source": [
        "## Subjectivity\n",
        "\n",
        "As well as providing a Sentiment score, TextBlob provides a Subjectivity score (between 0 and 1).  Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.\n",
        "\n",
        "We can improve the previous code by including a Subjectivity score. We will also do a small calculation to provide a textual indication of the overall Sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Note 1: I've added type annotations to these functions, here indicating that\n",
        "# both arguments are `float` values, and the function returns a `str`. These\n",
        "# are useful, in that they give us a note of what kinds of data to pass to\n",
        "# our functions, and what sort of data to expect back - but they are also\n",
        "# completely optional, and you can ignore them if you prefer \n",
        "# \n",
        "# Note 2: the second argument, `eta`, has a default value of 0.000001. This \n",
        "# means that we do not need to provide a value of `eta` when the function is\n",
        "# called - the default will be used if no value is provided \n",
        "def classify_sentiment(polarity: float, eta: float=1e-6) -> str:\n",
        "    \"\"\"Recieves a polarity value and classifies it as 'Positive', 'Neutral', or\n",
        "    'Negative'. For a 'Neutral' classification, rather than require polarity to \n",
        "    equal zero exactly, we require it to approximately equal zero, with \n",
        "    tolerances given by `eta`\n",
        "    \"\"\"\n",
        "    if polarity > eta:\n",
        "        return \"Positive\"\n",
        "    elif polarity < -eta:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "def simple_sa(input_text: str) -> tuple[float, float]:\n",
        "    \"\"\"Takes a sample text and returns polarity and subjectivity values\"\"\"\n",
        "    blob = TextBlob(input_text) #pass the input text to TextBlob\n",
        "    polarity = blob.sentiment.polarity #get a polarity score\n",
        "    subjectivity = blob.sentiment.subjectivity #get a subjectivity score\n",
        "    return polarity, subjectivity\n",
        "\n",
        "def print_sa(polarity: float, subjectivity: float) -> None:\n",
        "    print(f'Polarity: {polarity:.2f}') # rounds value to 2 d.p. for display\n",
        "    print(f'Subjectivity: {subjectivity:.2f}')\n",
        "    print(f'Sentiment: {classify_sentiment(polarity)}')\n",
        "\n",
        "polarity, subjectivity = simple_sa(\"the tiger is a different feline\") #(Donald Trump, Nov 13, 2020)\n",
        "\n",
        "# polarity, subjectivity = simple_sa(\"This is a terrific book, the writing is superb, overall an excellent read\") # (Amazon review)\n",
        "\n",
        "print_sa(polarity, subjectivity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szuUQEpA8FVe"
      },
      "source": [
        "## Working with a text file\n",
        "\n",
        "It is useful to be able to analyse a file rather than a string of text.\n",
        "\n",
        "Our first dataset is here\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/darwin-origin.txt\" download>https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/darwin-origin.txt</a>\n",
        "\n",
        "We can import it directly from GitHub\n",
        "\n",
        "You can see how different books scores by changing which .txt file you are processing. go to <a href=\"https://github.com/DCS-training/SentimentAnalysis/blob/main/README.md\" download>https://github.com/DCS-training/SentimentAnalysis/blob/main/README.md </a>\n",
        "\n",
        "and see which other .txt files are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFdRnsa687Td"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/darwin-origin.txt'\n",
        "darwin_origin = pd.read_fwf(url)\n",
        "darwin_origin =darwin_origin.to_string()#Transform the dataframe into a string\n",
        "print(darwin_origin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhYrSqe68FVf"
      },
      "source": [
        "## Analyse the text file\n",
        "\n",
        "Now we can change our previous code to open a text file rather than reading a string of text.\n",
        "\n",
        "Notice in the following example I have used the Python 'round' function to convert the results to 2 decimal places.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg8pc6bf8FVg"
      },
      "outputs": [],
      "source": [
        "# Note that we are here re-using functions created in an earlier\n",
        "# cell. That's the beauty of functions - they can be written once\n",
        "# and used many times. However, as a reminder, here is the function\n",
        "# code.\n",
        "\n",
        "# def simple_sa(input_text: str) -> tuple[float, float]:\n",
        "#     \"\"\"Takes a sample text and returns polarity and subjectivity values\"\"\"\n",
        "#     blob = TextBlob(input_text) #pass the input text to TextBlob\n",
        "#     polarity = blob.sentiment.polarity #get a polarity score\n",
        "#     subjectivity = blob.sentiment.subjectivity #get a subjectivity score\n",
        "#     return polarity, subjectivity\n",
        "\n",
        "# def print_sa(polarity: float, subjectivity: float) -> None:\n",
        "#     print(f'Polarity: {polarity:.2f}') # rounds value to 2 d.p. for display\n",
        "#     print(f'Subjectivity: {subjectivity:.2f}')\n",
        "#     print(f'Sentiment: {classify_sentiment(polarity)}')\n",
        "\n",
        "polarity, subjectivity = simple_sa(darwin_origin) #get polarity & subjectivity scores\n",
        "print_sa(polarity, subjectivity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZwgfPr78FVg"
      },
      "source": [
        "### Try diffent text files\n",
        "\n",
        "Experiment by analysing different text files. A selection can be found on the workshop home page (or use a file of you own choosing):\n",
        "\n",
        "[https://github.com/DCS-training/SentimentAnalysis/blob/main/README.md](https://github.com/DCS-training/SentimentAnalysis/blob/main/README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIOks5Kz8FVg"
      },
      "source": [
        "## CSV Files\n",
        "\n",
        "You will often have data contained in a CSV file that you wish to analyse, this could be: the results of a survey, export from a database, collection of tweets.\n",
        "\n",
        "The following code example takes as its input a CSV file containing all Donald Trump's tweets in 2020, analyses each one for sentiment and creates a new CSV file containing the original text plus two new columns containing the Sentiment and Polarity.\n",
        "\n",
        "We can access the examples CSV files directly from GitHub \n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/trump-tweet-archive.csv\" download>https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/trump-tweet-archive.csv</a>\n",
        "\n",
        "If you are using google colab, to see the newly created file you should go on the file explorer on the left handside and you will see the newly created.csv file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/trump-tweets-2020.csv'\n",
        "trump_tweets = pd.read_csv(url)\n",
        "\n",
        "# Let's see what we've got here:\n",
        "print(trump_tweets.dtypes) # what does `pandas` *think* the data-types are\n",
        "trump_tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's set the data-types in our dataframe to match the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trump_tweets['id'] = trump_tweets['id'].astype(int)\n",
        "trump_tweets['created_at'] = pd.to_datetime(trump_tweets['created_at'], format='%d/%m/%Y %H:%M')\n",
        "trump_tweets['text'] = trump_tweets['text'].astype('string')\n",
        "print(trump_tweets.dtypes) # just checking\n",
        "trump_tweets.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtbeyUHf8FVh"
      },
      "outputs": [],
      "source": [
        "\n",
        "out_file = \"trump-tweets-2020-sentiment.csv\"\n",
        "\n",
        "# On reflection, it would be better not to have our code for\n",
        "# calculating subjectivity bundled together with polarity in \n",
        "# one function: a function should do just one job \n",
        "\n",
        "def get_subjectivity(text: str) -> float:\n",
        "    \"\"\"Returns the polarity of a text\"\"\"\n",
        "    return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "# Also, this is a bit nicer than the way we wrote `get_polarity`:\n",
        "# in stead of having variables whose only job is to carry a value\n",
        "# from one line to the next, we can write a lottle more tersely \n",
        "# and do the whole thing with one line of code. Let's re-write\n",
        "# `get_polarity` in this style\n",
        "\n",
        "def get_polarity(text: str) -> float:\n",
        "    \"\"\"Returns the polarity of a text\"\"\"\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Now, we can use pandas' `apply` method to apply our sentiment \n",
        "# analysis functions to the data and add columns for polarity, \n",
        "# subjectivty, and sentiment\n",
        "def dataset_sa(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    data['polarity'] = data['text'].apply(get_polarity).astype(float)\n",
        "    data['subjectivity'] = data['text'].apply(get_subjectivity).astype(float)\n",
        "    data['sentiment'] = data['polarity'].apply(classify_sentiment)\n",
        "    return data\n",
        "\n",
        "trump_tweets = dataset_sa(trump_tweets)\n",
        "trump_tweets.to_csv(out_file)\n",
        "\n",
        "print (f'{len(trump_tweets)} tweets analysed for sentiment - results written to {out_file}')\n",
        "trump_tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTKvkHx-8FVh"
      },
      "source": [
        "## Creating a Pie Chart of Sentiment of this CSV file\n",
        "\n",
        "Now we have scored each tweet for sentiment, using the Python Library 'MatPlotLib' it is easy to visualise the aggregate sentiment in this CSV file.\n",
        "\n",
        "The following code block, counts up the total number of each positive, negative and neutral tweets and outputs the result as a Pie Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HMvwJS48FVi"
      },
      "outputs": [],
      "source": [
        "# We don't need to load the data again - we already have it stored as `trump_tweets`:\n",
        "# but in case anyone is running these in separate sessions:\n",
        "# trump_tweets = pd.read_csv('trump-tweets-2020-sentiment.csv')\n",
        "\n",
        "sentiment_counts = dict(trump_tweets['sentiment'].value_counts())\n",
        "\n",
        "# print the totals\n",
        "for k, v in sentiment_counts.items():\n",
        "    print(f'{k}: {v}') \n",
        "\n",
        "# and make a nice pie chart \n",
        "colours = [\"green\", \"red\", \"orange\"]\n",
        "plt.pie(\n",
        "    sentiment_counts.values(), \n",
        "    labels=sentiment_counts.keys(), \n",
        "    colors=colours,\n",
        "    autopct='%1.1f%%', \n",
        "    shadow=True, \n",
        "    startangle=90)\n",
        "plt.title(\"Sentiment Analysis of Trump Tweets\")\n",
        "plt.show()\n",
        "\n",
        "# Bonus task: can you do a similar analysis of just the retweets? Or everything but the retweets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--nL__DS8FVi"
      },
      "source": [
        "## Sentiment by Keyword\n",
        "\n",
        "Now that we have a CSV file scored for sentiment we can search the file for a particular keyword. The following code searches the Tweets in our CSV file for a keyword and for every hit, prints out the text with a polarity score at the end, and also provides an overall sentiment score. Try changing the originsl keyword to one of your own choosing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRZUo1J48FVj"
      },
      "outputs": [],
      "source": [
        "# We don't need to load the data again - we already have it stored as `trump_tweets`:\n",
        "# but in case anyone is running these in separate sessions:\n",
        "# trump_tweets = pd.read_csv('trump-tweets-2020-sentiment.csv')\n",
        "\n",
        "keyword = \"dog\"\n",
        "\n",
        "def bracket_kw(text: str, kw: str) -> str:\n",
        "    \"\"\"Highlights keyword by placing it in square brackets\"\"\"\n",
        "    return text.replace(keyword, f\"[{keyword}]\")\n",
        "\n",
        "def sentiment_by_kw(dataset: pd.DataFrame, keyword:str, verbose=True) -> tuple[int, float]:\n",
        "    count = 0\n",
        "    polarityscore = 0.0\n",
        "    keyword = keyword.lower()\n",
        "    for row in dataset.iloc:\n",
        "\n",
        "        text = row['text']\n",
        "\n",
        "        if keyword in text.lower():\n",
        "\n",
        "            count += 1\n",
        "            polarityscore = polarityscore + row['polarity']\n",
        "            if verbose:\n",
        "                print(f\"{count}. {bracket_kw(text, keyword)}, {row['polarity']:.2f}\\n\")\n",
        "    if count > 0: # Make sure we don't get a ZeroDivisionError\n",
        "        return count, polarityscore/count\n",
        "    else:\n",
        "        return count, None \n",
        "\n",
        "def display_kw_sa(count: int, polarityscore: float) -> None:\n",
        "    if count > 0:\n",
        "        avgpolarity = polarityscore\n",
        "        avgsentiment = classify_sentiment(avgpolarity)\n",
        "\n",
        "        print ('==================================================')\n",
        "        print (f'{count} occurences of \"{keyword.lstrip()}\" found in text')\n",
        "        print (f'Average Sentiment: {avgsentiment}')\n",
        "        print (f'Average Polarity: {avgpolarity:.3f}')\n",
        "        print ('==================================================')\n",
        "\n",
        "    else:\n",
        "        print ('==================================================')\n",
        "        print (f'No occurences of {keyword} found in text')\n",
        "        print ('==================================================')\n",
        "\n",
        "count, polarityscore = sentiment_by_kw(trump_tweets, keyword)\n",
        "display_kw_sa(count, polarityscore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXK4zUf78FVj"
      },
      "source": [
        "## Analysing a text file by keyword\n",
        "\n",
        "We can do a similar analysis by keyword of a text file. Here we will use the 'origin-intro.txt' we used earlier. This time the code will read in the sentence by sentence so we can analyse the keyword in context rather than just provide an overall result.\n",
        "\n",
        "Also, unlike the Trump Sentiment CSV we have not pre-analysed the text so we will do this 'on the fly' with TextBlob as we read each line. Try changing the keyword, but also experiment with different text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwfgH3ky8FVk"
      },
      "outputs": [],
      "source": [
        "\n",
        "searchterm = 'natural' # Choose a term to search for\n",
        "url = 'https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/darwin-origin.txt'\n",
        "#Import file \n",
        "\n",
        "# A couple of functions to get text data and process it into a dataframe. Note that I've \n",
        "# separated these into two functions, as later we will need these jobs to be done separately.\n",
        "# It's always better to write more, simpler functions, rather than fewer, more complicated\n",
        "def text_from_url(url: str) -> str:\n",
        "    # read file from url\n",
        "    input_file = pd.read_fwf(url)\n",
        "    # Transform the dataframe into a string\n",
        "    input_text = input_file.to_string() \n",
        "    # change input text to lower case\n",
        "    text = input_text.lower()\n",
        "    return text\n",
        "\n",
        "def text_to_df(text: str) -> pd.DataFrame:\n",
        "    # split the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # and reeturn as a dataframe\n",
        "    return pd.DataFrame({'text': sentences})\n",
        "\n",
        "# get the data\n",
        "darwin_origin_df = text_to_df(text_from_url(url))\n",
        "# analyse sentiment - remember, this function adds sentiment columns to the DataFrame\n",
        "darwin_origin_df = dataset_sa(darwin_origin_df)\n",
        "\n",
        "# analyse by keyword. Note that by structuring our code with functions, we can easily reuse it\n",
        "count, polarityscore = sentiment_by_kw(darwin_origin_df, keyword)\n",
        "display_kw_sa(count, polarityscore)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmMdw_F98FVk"
      },
      "source": [
        "## Analysing a text by sections\n",
        "\n",
        "Often you will want to analyse the different sections of a text to compare sentiment throughout. For example, analysing the sentiment of different chapters in a novel. To do this, it is necessary to identify in the text the various sections you wish to examine. Although it is often possible to use Python to do this, because the structure of documents can vary it is often easier to mark up the document manually. In the following example, I have taken a novel (Pride and Prejudice by Jane Austen) and inserted `<chapter></chapter>` tags to indicate where a chapter begins and ends.\n",
        "\n",
        "```\n",
        "\n",
        "<chapter>\n",
        "    CHAPTER 1\n",
        "    It is a truth universally acknowledged, that a single man in possession\n",
        "    of a good fortune, must be in want of a wife... \n",
        "</chapter>\n",
        "\n",
        "<chapter>\n",
        "    CHAPTER 2\n",
        "    Mr. Bennet was among the earliest of those who waited on Mr. Bingley...\n",
        "</chapter>\n",
        "\n",
        "```\n",
        "\n",
        "You can download it from here\n",
        "\n",
        "<a href=\"https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/austen-pride-prejudice.txt\" download>https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/austen-pride-prejudice.txt</a>\n",
        "\n",
        "### Analyse by Chapter\n",
        "\n",
        "Once you have uploaded the Pride and Prejudice text run the following code block.\n",
        "\n",
        "This will analyse each chapter for sentiment, print out a score for each chapter and an overall score. I will also create a csv file (named the same as the input file but with a .csv extension) that can be used to create avisualisation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPSWbA048FVl"
      },
      "outputs": [],
      "source": [
        "text_url = 'https://raw.githubusercontent.com/DCS-training/SentimentAnalysis/main/austen-pride.txt'\n",
        "csv_name = 'austen-pride.csv'\n",
        "\n",
        "# Download text file from URL and read its contents\n",
        "response = urllib.request.urlopen(text_url)\n",
        "austen_pride = response.read().decode('utf-8')\n",
        "\n",
        "# writeFile = csv.writer(open(csvName, \"w\"))\n",
        "\n",
        "\n",
        "title = re.findall('<title>(.*?)</title>', austen_pride)\n",
        "author = re.findall('<author>(.*?)</author>', austen_pride, re.DOTALL)\n",
        "\n",
        "def chapterise(book: str) -> tuple[list[str]]:\n",
        "    return re.findall('<chapter>(.*?)</chapter>', book, re.DOTALL)\n",
        "\n",
        "\n",
        "\n",
        "def analyse_chapter(chapter):\n",
        "    \"\"\"breaks a chapter into sentences, runs sentiment analysis on the sentences,\n",
        "    and gives the mean sentiment\n",
        "    \"\"\"\n",
        "    sentence_data = text_to_df(chapter)\n",
        "    sentence_data = dataset_sa(sentence_data)\n",
        "    chap_score = sentence_data['polarity'].mean()\n",
        "    return chap_score\n",
        "\n",
        "def analyse_by_chapters(book: str) -> pd.DataFrame:\n",
        "    chapters = chapterise(book)\n",
        "    df = pd.DataFrame({\n",
        "        'chapter': list(range(1, len(chapters)+1)), # why not `list(range(len(chapters)))`?\n",
        "        'polarity': [analyse_chapter(chapter) for chapter in chapters] \n",
        "    }) # 'polarity' is set using a *list comprehension* - a quite nice technique\n",
        "    # for writing an entire python for-loop in a single line\n",
        "    df['sentiment'] = df['polarity'].apply(classify_sentiment)\n",
        "    return df\n",
        "\n",
        "chapters_df = analyse_by_chapters(austen_pride)\n",
        "chapters_df.to_csv(csv_name)\n",
        "print (f'csv created - {csv_name}')\n",
        "\n",
        "\n",
        "\n",
        "def disp_chap_analysis(df: pd.DataFrame, title: list[str], author: list[str]) -> None:\n",
        "    for row in df.iloc:\n",
        "        print(f'{row['chapter']}: {row['polarity']:.3f}, {row['sentiment']}')\n",
        "    avgSent = df['polarity'].mean()\n",
        "    print ('***************************************')\n",
        "    print (' '.join(title))\n",
        "    print ('by')\n",
        "    print (' '.join(author))\n",
        "    print (f'Average chapter sentiment = {avgSent:.3f}')\n",
        "    print ('***************************************')\n",
        "\n",
        "disp_chap_analysis(chapters_df, title, author)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_z_c-Gg8FVl"
      },
      "source": [
        "## Create a Barchart to illustrate results\n",
        "\n",
        "Using the CSV file created with last piece of code we can create a visualisation. \n",
        "\n",
        "Before running the next code block, from the top Noteable menu select: Cell > All Output > Clear\n",
        "\n",
        "Run the following code block to import the CSV and create a barchart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zYWW8JU8FVm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "\n",
        "plt.bar(\n",
        "    x=chapters_df['chapter'],\n",
        "    height=chapters_df['polarity']\n",
        ")\n",
        "\n",
        "plt.title('Pride and Prejudice - Sentiment by Chapter')\n",
        "\n",
        "plt.xlabel('Chapter')\n",
        "plt.ylabel('Polarity')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZaw0KyZ8FVm"
      },
      "source": [
        "## Further Exercises\n",
        "\n",
        "Experiment by analysing different text files. A selection can be found on this GitHub Repo\n",
        "You can either save them on your pc and import them or import them directly from the GitHub Repo. If you are using Noteable, once the file has been saved to your computer, go back to the Noteable home tab in the browser.\n",
        "\n",
        "* Select 'Upload' from the top right of the page. \n",
        "* Browse to the file.\n",
        "* Click 'Select'\n",
        "* Click on the blue 'Upload' button\n",
        "\n",
        "The file is now available to be used in Noteable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX0O3m7b8FVm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
